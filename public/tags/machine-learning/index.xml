<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on jwhogg</title>
    <link>http://localhost:1313/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on jwhogg</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Edge AI: ML inference in the browser</title>
      <link>http://localhost:1313/articles/ml_inference_browser/</link>
      <pubDate>Tue, 18 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/ml_inference_browser/</guid>
      <description>Recently, I stumbled across a guy who ported OpenAI&amp;rsquo;s Whisper model into c++, in various sizes, allowing the model to be run on-device, at impressive speed.&#xA;(source) {{ youtube TCMdywaJ9iY}} I went down a rabbit-hole, and found a whole family of popular models that had been ported to work on-device, from the browser:&#xA;Stable difusion running in browser Text emotion prediction in browser Llama c++ Web LLM YOLO in the browser Edge Computing This movement is part of the larger Edge computing trend, which focuses on bringing computing as close to the source as possible, to reduce latency.</description>
    </item>
    <item>
      <title>Word2Vec Overview</title>
      <link>http://localhost:1313/articles/word2vec_intro/</link>
      <pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/word2vec_intro/</guid>
      <description>In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released. We will also go into the training of the neural network, so it is assumed you have some knowledge on this.&#xA;These 2 papers introduced word2vec to the world back in 2013: Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements Motivation For many NLP tasks, we need to learn on data which can&amp;rsquo;t be easily represented numerically.</description>
    </item>
  </channel>
</rss>
