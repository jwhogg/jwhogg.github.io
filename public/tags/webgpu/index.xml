<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>WebGPU on jwhogg</title>
    <link>http://localhost:1313/tags/webgpu/</link>
    <description>Recent content in WebGPU on jwhogg</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/webgpu/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Edge AI: ML inference in the browser</title>
      <link>http://localhost:1313/articles/ml_inference_browser/</link>
      <pubDate>Tue, 18 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/ml_inference_browser/</guid>
      <description>Recently, I stumbled across a guy who ported OpenAI&amp;rsquo;s Whisper model into c++, in various sizes, allowing the model to be run on-device, at impressive speed.&#xA;I went down a rabbit-hole, and found a whole family of popular models that had been ported to work on-device, from the browser:&#xA;Stable difusion running in browser Text emotion prediction in browser Llama c++ Web LLM YOLO in the browser Edge Computing This movement is part of the larger Edge computing trend, which focuses on bringing computing as close to the source as possible, to reduce latency.</description>
    </item>
  </channel>
</rss>
