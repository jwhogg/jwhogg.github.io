<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width"><meta name="description" content="" />

<title>
    
    Implementing Word2Vec in python | jwhogg
    
</title>







<link rel="stylesheet" href="/assets/combined.min.f69b4a5b91a9bc416804eda1f19d04210551b843d47318bf1913d7edce1504fb.css" media="all">



  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">
    <h1 class="header-title">jwhogg</h1>

    <div class="flex">
        

        
    </div>

</div>
      </header>

      <main class="main">
        





<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Implementing Word2Vec in python</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-05-31T17:25:04&#43;01:00">May 31, 2024</time>
      

      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <p>We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.</p>
<p>Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window.











<figure class="">

    <div>
        <img loading="lazy" alt="Neural Network for Word2Vec" src=" /images/word2vec_nn_diagram.png">
    </div>

    
</figure></p>
<p>The error function we want to minimise:</p>
<ul>
<li>This is negaitve log likelihood, but for our purposes is same as cross entropy loss
$$ 1/T  \sum\limits_{-c\le j\le c,j\ne 0}^{T} \log P(w_{t+j}|w_{t}) $$</li>
</ul>
<p>Here is the derivation of cross-entropy loss for backprop:
$$ \frac{\partial \mathcal{L}}{\partial A_2} = Z - y $$
so all we need to do is $Z-y$ (where $Z$ is output of NN) when we do backprop</p>
<p>Algebraic form of the NN model:
$$ A_1 = XW_1 \newline A_2 = A_1W_2 \newline Z = softmax(A_2) $$</p>
<p>where:</p>
<ul>
<li>$X$ is the input vector (for CBOW, a word from the context window)</li>
<li>$W_{1}$ is weight matrix for the input-&gt;hidden layer</li>
<li>$Z$ is the output of the NN (probabilities of each word)</li>
</ul>
<h5 id="pseudocode-for-the-models-forward-method">Pseudocode for the model&rsquo;s forward() method:</h5>
<p>(where @ is dot product)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a1 <span style="color:#ff79c6">=</span> X <span style="color:#ff79c6">@</span> w1
</span></span><span style="display:flex;"><span>a2 <span style="color:#ff79c6">=</span> a1 <span style="color:#ff79c6">@</span> w2
</span></span><span style="display:flex;"><span>z <span style="color:#ff79c6">=</span> softmax(a2)
</span></span></code></pre></div><p>Softmax function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">softmax</span>(X):
</span></span><span style="display:flex;"><span>    res <span style="color:#ff79c6">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">for</span> x <span style="color:#ff79c6">in</span> X:
</span></span><span style="display:flex;"><span>        exp <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>exp(x)
</span></span><span style="display:flex;"><span>        res<span style="color:#ff79c6">.</span>append(exp <span style="color:#ff79c6">/</span> exp<span style="color:#ff79c6">.</span>sum())
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> res
</span></span></code></pre></div><h5 id="pseudocode-for-backprop">Pseudocode for backprop:</h5>
<p>remember that our loss equation worked out to be $Z - y$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>da2 <span style="color:#ff79c6">=</span> Z <span style="color:#ff79c6">-</span> y
</span></span><span style="display:flex;"><span>dw2 <span style="color:#ff79c6">=</span> a1<span style="color:#ff79c6">.</span>T <span style="color:#ff79c6">@</span> da2
</span></span><span style="display:flex;"><span>da1 <span style="color:#ff79c6">=</span> da2 <span style="color:#ff79c6">@</span> w2<span style="color:#ff79c6">.</span>T
</span></span><span style="display:flex;"><span>dw1 <span style="color:#ff79c6">=</span> X<span style="color:#ff79c6">.</span>T <span style="color:#ff79c6">@</span> da1
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">assert</span>(dw2<span style="color:#ff79c6">.</span>shape<span style="color:#ff79c6">==</span>w2<span style="color:#ff79c6">.</span>shape)<span style="color:#6272a4">#ensure same shape before update weights</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">assert</span>(dw1<span style="color:#ff79c6">.</span>shape<span style="color:#ff79c6">==</span>w1<span style="color:#ff79c6">.</span>shape)
</span></span><span style="display:flex;"><span><span style="color:#6272a4">#updating weights in model:</span>
</span></span><span style="display:flex;"><span>w1 <span style="color:#ff79c6">-=</span> alpha <span style="color:#ff79c6">*</span> dw1
</span></span><span style="display:flex;"><span>w2 <span style="color:#ff79c6">-=</span> alpha <span style="color:#ff79c6">*</span> dw2
</span></span><span style="display:flex;"><span>CEL <span style="color:#ff79c6">=</span> crossentropy(Z,y) <span style="color:#6272a4">#for logging purposes (I think)</span>
</span></span></code></pre></div><p>where:
<code>da2</code> is the derivative of a2
<code>alpha</code> is learning rate
cross entropy is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">cross_entropy</span>(z, y):
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> <span style="color:#ff79c6">-</span> np<span style="color:#ff79c6">.</span>sum(np<span style="color:#ff79c6">.</span>log(z) <span style="color:#ff79c6">*</span> y)
</span></span></code></pre></div><p>default hyper-params:</p>
<ul>
<li>iterations: 50</li>
<li>learning rate: 0.05</li>
</ul>
<h4 id="sources">Sources:</h4>
<p><a href="https://jaketae.github.io/study/word2vec/">Code based on work by Jake Tae</a><br>
<a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Great detailed explanation of Word2Vec by Chris McCormik</a><br>
<a href="https://www.youtube.com/watch?v=ERibwqs9p38">Stanford lecture on Word2Vec, which gives more detail on the motivation</a></p>

    
  </div>

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      <p>Powered by
    <a href="https://gohugo.io/">Hugo</a>
    and
    <a href="https://github.com/tomfran/typo">tomfran/typo</a>
</p>


<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ]
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>