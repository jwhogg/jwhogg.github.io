<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Implementing Word2Vec in python - jwhogg
  </title>

  <meta name="description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:" /><meta name="generator" content="Hugo 0.126.1">

  <link rel="stylesheet" href="http://localhost:1313/css/main.css" />

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"

    onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>



  
  

  <meta property="og:url" content="http://localhost:1313/articles/word2vec_code_overview/">
  <meta property="og:site_name" content="jwhogg">
  <meta property="og:title" content="Implementing Word2Vec in python">
  <meta property="og:description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2024-05-31T17:25:04+01:00">
    <meta property="article:modified_time" content="2024-05-31T17:25:04+01:00">
    <meta property="og:image" content="http://localhost:1313/digital-garden-logo.png">


  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/digital-garden-logo.png">
  <meta name="twitter:title" content="Implementing Word2Vec in python">
  <meta name="twitter:description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">


  
  <meta itemprop="name" content="Implementing Word2Vec in python">
  <meta itemprop="description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">
  <meta itemprop="datePublished" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="dateModified" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="wordCount" content="350">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">

  
  <meta itemprop="name" content="Implementing Word2Vec in python">
  <meta itemprop="description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">
  <meta itemprop="datePublished" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="dateModified" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="wordCount" content="350">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">
</head><head>
  
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Implementing Word2Vec in python - jwhogg
  </title>

  <meta name="description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:" /><meta name="generator" content="Hugo 0.126.1">

  <link rel="stylesheet" href="http://localhost:1313/css/main.css" />

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"

    onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>



  
  

  <meta property="og:url" content="http://localhost:1313/articles/word2vec_code_overview/">
  <meta property="og:site_name" content="jwhogg">
  <meta property="og:title" content="Implementing Word2Vec in python">
  <meta property="og:description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2024-05-31T17:25:04+01:00">
    <meta property="article:modified_time" content="2024-05-31T17:25:04+01:00">
    <meta property="og:image" content="http://localhost:1313/digital-garden-logo.png">


  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/digital-garden-logo.png">
  <meta name="twitter:title" content="Implementing Word2Vec in python">
  <meta name="twitter:description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">


  
  <meta itemprop="name" content="Implementing Word2Vec in python">
  <meta itemprop="description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">
  <meta itemprop="datePublished" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="dateModified" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="wordCount" content="350">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">

  
  <meta itemprop="name" content="Implementing Word2Vec in python">
  <meta itemprop="description" content="We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:">
  <meta itemprop="datePublished" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="dateModified" content="2024-05-31T17:25:04+01:00">
  <meta itemprop="wordCount" content="350">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">
</head>
</head>
<body class="flex relative h-full min-h-screen"><aside
  class="will-change-transform transform transition-transform -translate-x-full absolute top-0 left-0 md:relative md:translate-x-0 w-3/4 md:basis-60 h-full min-h-screen p-3 bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 flex flex-col gap-2.5 z-20 sidebar flex-shrink-0">
  <p class="font-bold mb-5 flex items-center gap-2">
    <button aria-label="Close sidebar"
      class="md:hidden menu-trigger-close p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-6 w-6" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
</svg></button>
    <a href="http://localhost:1313/" class="px-2">
      <span>jwhogg</span>
    </a>
    <button aria-label="Toggle dark mode"
      class="dark-mode-toggle p-2 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <circle cx="12" cy="12" r="4" />
  <path d="M3 12h1M12 3v1M20 12h1M12 20v1M5.6 5.6l.7 .7M18.4 5.6l-.7 .7M17.7 17.7l.7 .7M6.3 17.7l-.7 .7" />
</svg></button>
  </p>

  
  <ul class="list-none flex flex-col gap-1">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/" >
        <span>Home</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  text-slate-400 font-semibold pb-0 pl-1 border-b cursor-default pointer-events-none "
        href="#" >
        <span>Content</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles" >
        <span>Articles</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/portfolio" >
        <span>Portfolio</span>
        
      </a>
    </li>
    
  </ul>

  <div class="flex-1"></div>

  

  <ul class="list-none flex flex-wrap justify-center gap-1 pt-2 border-t border-slate-200 dark:border-slate-600">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="https://github.com/jwhogg" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">GitHub</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path
    d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22" />
</svg></span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="https://linkedin.com/in/joel-w-hogg" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">LinkedIn</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z" />
  <rect x="2" y="9" width="4" height="12" />
  <circle cx="4" cy="4" r="2" />
</svg></span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="mailto:joelhogg45@gmail.com" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">Email</span>
        
        <span><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <rect x="3" y="5" width="18" height="14" rx="2" />
  <polyline points="3 7 12 13 21 7" />
</svg></span>
        
      </a>
    </li>
    
  </ul>
</aside>

<div
  class="fixed bg-slate-700 bg-opacity-5 transition duration-200 ease-in-out inset-0 z-10 pointer-events-auto md:hidden left-0 top-0 w-full h-full hidden menu-overlay">
</div>

<button aria-label="Toggle Sidebar"
  class="md:hidden absolute top-3 left-3 z-10 menu-trigger p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-100"><svg class="h-6 w-6" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="4" y1="6" x2="20" y2="6" />
  <line x1="4" y1="12" x2="20" y2="12" />
  <line x1="4" y1="18" x2="16" y2="18" />
</svg></button>






<div class="flex flex-1 h-screen relative w-full min-w-0">

  <section
    class="will-change-transform transform transition-transform -translate-x-[200%] absolute top-0 left-0 lg:relative
  lg:translate-x-0 lg:basis-[400px] h-full bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 lg:flex flex-col py-3 overflow-y-auto scroll-area flex-shrink-0">
    
    
    <a href="http://localhost:1313/articles/">
      <h2 class="font-bold mb-5 py-1 pl-12 pr-3 md:px-3">Articles</h2>
    </a>
<label class="mb-2 block px-2">
  <span></span>
  <input type="text" placeholder="Search articles" class="searchInput bg-white rounded px-2 py-1 w-full border" />
</label>
<script>
  const searchElt = document.querySelector('.searchInput')
  if (searchElt) {
    searchElt.addEventListener('input', evt => {
      const value = evt.target.value || '';

      const articles = document.querySelectorAll('.article')
      if (!value) {
        articles.forEach(article => article.classList.remove('hidden'))
      } else {
        articles.forEach(article => article.classList.add('hidden'))
        Array.from(articles).filter(article => {
          const title = article.querySelector('h3')
          if (!title) return false;

          return title.textContent.toLowerCase().includes(value.toLowerCase())
        }).forEach(article => article.classList.remove('hidden'))
      }
    })
  }
</script>
<div class="space-y-2.5">
      
      <a class="article block px-3 py-4  bg-slate-900 dark:bg-slate-700 text-slate-50 "
        href="/articles/word2vec_code_overview/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Implementing Word2Vec in python</h3>
        <div
          class="text-sm  text-slate-400  line-clamp-2">
          We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/article-3/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Fine-tuning a pre-trained model from HuggingFace</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          We will be using a 🤗 HuggingFace model (GPT-2 Medium)
📙Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/article-2/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Getting an ML job- notes from blogs</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          Summary of this article / video
Intro can split getting an ML job into 2 steps: Getting ML skills building projects contributing to OS reading technical info Marketing ML skills communication interviewing portal creation passing application screening Strategies: Make ML jobs come to you by Learning in Public (great article!) Summary:
write blogs / tutorials / cheatsheets answer things on stackoverflow / reddit make videos draw cartoons Pull Request libraries you use OS: github repo -&gt; issues -&gt; &lsquo;good first issue&rsquo; (easy ones to solve) make your own libraries intended only for you goto conferences ==&gt; make the thing you wish you had found Info
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/article-1/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">My First Post</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          testing
        </div>
      </a>
      </div>
    </section>

  <div class="overflow-y-auto h-screen w-full">
    <article class="px-6 py-20 w-full mx-auto prose lg:prose-lg h-fit dark:prose-invert prose-img:mx-auto">

      
      <h1 class="!mb-2">Implementing Word2Vec in python</h1>

	  
      <p class="text-sm text-slate-500 !mb-8"> Planted May 31, 2024
		
	  </p>
      

      

      <p>We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.</p>
<p>Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window.
<img src="/images/word2vec_nn_diagram.png" alt="Neural Network for Word2Vec"></p>
<p>The error function we want to minimise:</p>
<ul>
<li>I think this is negaitve log likelihood, but for our purposes is same as cross entropy loss
$$ 1/T  \sum\limits_{-c\le j\le c,j\ne 0}^{T} \log P(w_{t+j}|w_{t}) $$</li>
</ul>
<p>Here is the derivation of cross-entropy loss for backprop:
$$ \frac{\partial \mathcal{L}}{\partial A_2} = Z - y $$
so all we need to do is $Z-y$ (where $Z$ is output of NN) when we do backprop</p>
<p>Algebraic form of the NN model:
$$ A_1 = XW_1 \newline A_2 = A_1W_2 \newline Z = softmax(A_2) $$</p>
<p>where:</p>
<ul>
<li>$X$ is the input vector (for CBOW, a word from the context window)</li>
<li>$W_{1}$ is weight matrix for the input-&gt;hidden layer</li>
<li>$Z$ is the output of the NN (probabilities of each word)</li>
</ul>
<h5 id="pseudocode-for-the-models-forward-method">Pseudocode for the model&rsquo;s forward() method:</h5>
<p>(where @ is dot product)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a1 <span style="color:#ff79c6">=</span> X <span style="color:#ff79c6">@</span> w1
</span></span><span style="display:flex;"><span>a2 <span style="color:#ff79c6">=</span> a1 <span style="color:#ff79c6">@</span> w2
</span></span><span style="display:flex;"><span>z <span style="color:#ff79c6">=</span> softmax(a2)
</span></span></code></pre></div><p>Softmax function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">softmax</span>(X):
</span></span><span style="display:flex;"><span>    res <span style="color:#ff79c6">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">for</span> x <span style="color:#ff79c6">in</span> X:
</span></span><span style="display:flex;"><span>        exp <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>exp(x)
</span></span><span style="display:flex;"><span>        res<span style="color:#ff79c6">.</span>append(exp <span style="color:#ff79c6">/</span> exp<span style="color:#ff79c6">.</span>sum())
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> res
</span></span></code></pre></div><h5 id="pseudocode-for-backprop">Pseudocode for backprop:</h5>
<p>remember that our loss equation worked out to be $Z - y$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>da2 <span style="color:#ff79c6">=</span> Z <span style="color:#ff79c6">-</span> y
</span></span><span style="display:flex;"><span>dw2 <span style="color:#ff79c6">=</span> a1<span style="color:#ff79c6">.</span>T <span style="color:#ff79c6">@</span> da2
</span></span><span style="display:flex;"><span>da1 <span style="color:#ff79c6">=</span> da2 <span style="color:#ff79c6">@</span> w2<span style="color:#ff79c6">.</span>T
</span></span><span style="display:flex;"><span>dw1 <span style="color:#ff79c6">=</span> X<span style="color:#ff79c6">.</span>T <span style="color:#ff79c6">@</span> da1
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">assert</span>(dw2<span style="color:#ff79c6">.</span>shape<span style="color:#ff79c6">==</span>w2<span style="color:#ff79c6">.</span>shape)<span style="color:#6272a4">#ensure same shape before update weights</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">assert</span>(dw1<span style="color:#ff79c6">.</span>shape<span style="color:#ff79c6">==</span>w1<span style="color:#ff79c6">.</span>shape)
</span></span><span style="display:flex;"><span><span style="color:#6272a4">#updating weights in model:</span>
</span></span><span style="display:flex;"><span>w1 <span style="color:#ff79c6">-=</span> alpha <span style="color:#ff79c6">*</span> dw1
</span></span><span style="display:flex;"><span>w2 <span style="color:#ff79c6">-=</span> alpha <span style="color:#ff79c6">*</span> dw2
</span></span><span style="display:flex;"><span>CEL <span style="color:#ff79c6">=</span> crossentropy(Z,y) <span style="color:#6272a4">#for logging purposes (I think)</span>
</span></span></code></pre></div><p>where:
<code>da2</code> is the derivative of a2
<code>alpha</code> is learning rate
cross entropy is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">cross_entropy</span>(z, y):
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> <span style="color:#ff79c6">-</span> np<span style="color:#ff79c6">.</span>sum(np<span style="color:#ff79c6">.</span>log(z) <span style="color:#ff79c6">*</span> y)
</span></span></code></pre></div><p>default hyper-params:</p>
<ul>
<li>iterations: 50</li>
<li>learning rate: 0.05</li>
</ul>
<h4 id="sources">Sources:</h4>
<p><a href="https://jaketae.github.io/study/word2vec/"  target="_blank" rel="noopener" >Code based on work by Jake Tae</a>
<a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"  target="_blank" rel="noopener" >Great detailed explanation of Word2Vec by Chris McCormik</a>
<a href="https://www.youtube.com/watch?v=ERibwqs9p38"  target="_blank" rel="noopener" >Stanford lecture on Word2Vec, which gives more detail on the motivation</a></p>

    </article>
  </div>

</div>


  
<script type="text/javascript" src="/main.js" defer></script>


  


</body>

</html>