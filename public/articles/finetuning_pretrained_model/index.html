<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Fine-tuning a pre-trained model from HuggingFace - jwhogg
  </title>

  <meta name="description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;]." /><meta name="generator" content="Hugo 0.126.1">

  <link rel="stylesheet" href="http://localhost:1313/css/main.css" />

  
  
  
  

  <meta property="og:url" content="http://localhost:1313/articles/finetuning_pretrained_model/">
  <meta property="og:site_name" content="jwhogg">
  <meta property="og:title" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta property="og:description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2024-05-24T13:29:04+01:00">
    <meta property="article:modified_time" content="2024-05-24T13:29:04+01:00">
    <meta property="og:image" content="http://localhost:1313/digital-garden-logo.png">


  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/digital-garden-logo.png">
  <meta name="twitter:title" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta name="twitter:description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">


  
  <meta itemprop="name" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta itemprop="description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">
  <meta itemprop="datePublished" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="dateModified" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="wordCount" content="316">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">

  
  <meta itemprop="name" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta itemprop="description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">
  <meta itemprop="datePublished" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="dateModified" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="wordCount" content="316">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">
</head><head>
  
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Fine-tuning a pre-trained model from HuggingFace - jwhogg
  </title>

  <meta name="description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;]." /><meta name="generator" content="Hugo 0.126.1">

  <link rel="stylesheet" href="http://localhost:1313/css/main.css" />

  
  
  
  

  <meta property="og:url" content="http://localhost:1313/articles/finetuning_pretrained_model/">
  <meta property="og:site_name" content="jwhogg">
  <meta property="og:title" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta property="og:description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2024-05-24T13:29:04+01:00">
    <meta property="article:modified_time" content="2024-05-24T13:29:04+01:00">
    <meta property="og:image" content="http://localhost:1313/digital-garden-logo.png">


  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/digital-garden-logo.png">
  <meta name="twitter:title" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta name="twitter:description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">


  
  <meta itemprop="name" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta itemprop="description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">
  <meta itemprop="datePublished" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="dateModified" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="wordCount" content="316">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">

  
  <meta itemprop="name" content="Fine-tuning a pre-trained model from HuggingFace">
  <meta itemprop="description" content="We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].">
  <meta itemprop="datePublished" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="dateModified" content="2024-05-24T13:29:04+01:00">
  <meta itemprop="wordCount" content="316">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">
</head>
</head>
<body class="flex relative h-full min-h-screen"><aside
  class="will-change-transform transform transition-transform -translate-x-full absolute top-0 left-0 md:relative md:translate-x-0 w-3/4 md:basis-60 h-full min-h-screen p-3 bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 flex flex-col gap-2.5 z-20 sidebar flex-shrink-0">
  <p class="font-bold mb-5 flex items-center gap-2">
    <button aria-label="Close sidebar"
      class="md:hidden menu-trigger-close p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-6 w-6" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
</svg></button>
    <a href="http://localhost:1313/" class="px-2">
      <span>jwhogg</span>
    </a>
    <button aria-label="Toggle dark mode"
      class="dark-mode-toggle p-2 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <circle cx="12" cy="12" r="4" />
  <path d="M3 12h1M12 3v1M20 12h1M12 20v1M5.6 5.6l.7 .7M18.4 5.6l-.7 .7M17.7 17.7l.7 .7M6.3 17.7l-.7 .7" />
</svg></button>
  </p>

  
  <ul class="list-none flex flex-col gap-1">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/" >
        <span>Home</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  text-slate-400 font-semibold pb-0 pl-1 border-b cursor-default pointer-events-none "
        href="#" >
        <span>Content</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles" >
        <span>Articles</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/portfolio" >
        <span>Portfolio</span>
        
      </a>
    </li>
    
  </ul>

  <div class="flex-1"></div>

  

  <ul class="list-none flex flex-wrap justify-center gap-1 pt-2 border-t border-slate-200 dark:border-slate-600">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="https://github.com/jwhogg" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">GitHub</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path
    d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22" />
</svg></span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="https://linkedin.com/in/joel-w-hogg" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">LinkedIn</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z" />
  <rect x="2" y="9" width="4" height="12" />
  <circle cx="4" cy="4" r="2" />
</svg></span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="mailto:joelhogg45@gmail.com" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">Email</span>
        
        <span><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <rect x="3" y="5" width="18" height="14" rx="2" />
  <polyline points="3 7 12 13 21 7" />
</svg></span>
        
      </a>
    </li>
    
  </ul>
</aside>

<div
  class="fixed bg-slate-700 bg-opacity-5 transition duration-200 ease-in-out inset-0 z-10 pointer-events-auto md:hidden left-0 top-0 w-full h-full hidden menu-overlay">
</div>

<button aria-label="Toggle Sidebar"
  class="md:hidden absolute top-3 left-3 z-10 menu-trigger p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-100"><svg class="h-6 w-6" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="4" y1="6" x2="20" y2="6" />
  <line x1="4" y1="12" x2="20" y2="12" />
  <line x1="4" y1="18" x2="16" y2="18" />
</svg></button>






<div class="flex flex-1 h-screen relative w-full min-w-0">

  <section
    class="will-change-transform transform transition-transform -translate-x-[200%] absolute top-0 left-0 lg:relative
  lg:translate-x-0 lg:basis-[400px] h-full bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 lg:flex flex-col py-3 overflow-y-auto scroll-area flex-shrink-0">
    
    
    <a href="http://localhost:1313/articles/">
      <h2 class="font-bold mb-5 py-1 pl-12 pr-3 md:px-3">Articles</h2>
    </a>
<label class="mb-2 block px-2">
  <span></span>
  <input type="text" placeholder="Search articles" class="searchInput bg-white rounded px-2 py-1 w-full border" />
</label>
<script>
  const searchElt = document.querySelector('.searchInput')
  if (searchElt) {
    searchElt.addEventListener('input', evt => {
      const value = evt.target.value || '';

      const articles = document.querySelectorAll('.article')
      if (!value) {
        articles.forEach(article => article.classList.remove('hidden'))
      } else {
        articles.forEach(article => article.classList.add('hidden'))
        Array.from(articles).filter(article => {
          const title = article.querySelector('h3')
          if (!title) return false;

          return title.textContent.toLowerCase().includes(value.toLowerCase())
        }).forEach(article => article.classList.remove('hidden'))
      }
    })
  }
</script>
<div class="space-y-2.5">
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/word2vec_intro/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Word2Vec in detail</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which can&rsquo;t be easily represented numerically.
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/word2vec_code_overview/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Implementing Word2Vec in python</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:
        </div>
      </a>
      
      <a class="article block px-3 py-4  bg-slate-900 dark:bg-slate-700 text-slate-50 "
        href="/articles/finetuning_pretrained_model/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Fine-tuning a pre-trained model from HuggingFace</h3>
        <div
          class="text-sm  text-slate-400  line-clamp-2">
          We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/getting_an_ml_jpb/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Getting an ML job- notes from blogs</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          Summary of this article / video
Intro can split getting an ML job into 2 steps: Getting ML skills building projects contributing to OS reading technical info Marketing ML skills communication interviewing portal creation passing application screening Strategies: Make ML jobs come to you by Learning in Public (great article!) Summary:
write blogs / tutorials / cheatsheets answer things on stackoverflow / reddit make videos draw cartoons Pull Request libraries you use OS: github repo -&gt; issues -&gt; &lsquo;good first issue&rsquo; (easy ones to solve) make your own libraries intended only for you goto conferences ==&gt; make the thing you wish you had found Info
        </div>
      </a>
      </div>
    </section>

  <div class="overflow-y-auto h-screen w-full">
    <article class="px-6 py-20 w-full mx-auto prose lg:prose-lg h-fit dark:prose-invert prose-img:mx-auto">

      
      <h1 class="!mb-2">Fine-tuning a pre-trained model from HuggingFace</h1>

	  
      <p class="text-sm text-slate-500 !mb-8"> Planted May 24, 2024
		
	  </p>
      

      

      <p>We will be using a ðŸ¤— <a href="https://huggingface.co/"  target="_blank" rel="noopener" >HuggingFace</a> model (<a href="https://huggingface.co/openai-community/gpt2-medium"  target="_blank" rel="noopener" >GPT-2 Medium</a>)</p>
<h5 id="jupyter-notebookhttpsgithubcomjwhogggpt-2-fine-tuningblobmaingpt-220fine-tuning20cnndailymailipynb">ðŸ“™<strong><a href="https://github.com/jwhogg/GPT-2-Fine-Tuning/blob/main/GPT-2%20Fine-tuning%20CNNDailyMail.ipynb"  target="_blank" rel="noopener" >Jupyter Notebook</a></strong></h5>
<h3 id="create-traintest-split-for-custom-dataset">Create train/test split for custom dataset</h3>
<p>(can use sklearn for this)</p>
<h3 id="get-the-model-tokeniser">Get the model tokeniser</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> AutoTokenizer<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#34;your-model-here&#34;</span>) <span style="color:#6272a4">#eg &#34;bert-base-cased&#34;</span>
</span></span></code></pre></div><h3 id="create-encodings-for-traintest-using-tokeniser">Create encodings for train/test using tokeniser</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">tokenize_function</span>(examples):
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> tokenizer(examples[<span style="color:#f1fa8c">&#34;text&#34;</span>], padding<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;max_length&#34;</span>, truncation<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenized_datasets <span style="color:#ff79c6">=</span> raw_datasets<span style="color:#ff79c6">.</span>map(tokenize_function, batched<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># where raw_datasets is a dict with train/dev/test</span>
</span></span></code></pre></div><ul>
<li>we need padding as the inputs must fit the models input even if they are too short</li>
</ul>
<h5 id="create-small-datasets-for-development">Create small datasets for development</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>small_train_dataset <span style="color:#ff79c6">=</span> tokenized_datasets[<span style="color:#f1fa8c">&#34;train&#34;</span>]<span style="color:#ff79c6">.</span>shuffle(seed<span style="color:#ff79c6">=</span><span style="color:#bd93f9">42</span>)<span style="color:#ff79c6">.</span>select(<span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">1000</span>)) 
</span></span><span style="display:flex;"><span>small_eval_dataset <span style="color:#ff79c6">=</span> tokenized_datasets[<span style="color:#f1fa8c">&#34;test&#34;</span>]<span style="color:#ff79c6">.</span>shuffle(seed<span style="color:#ff79c6">=</span><span style="color:#bd93f9">42</span>)<span style="color:#ff79c6">.</span>select(<span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">1000</span>)) 
</span></span><span style="display:flex;"><span>full_train_dataset <span style="color:#ff79c6">=</span> tokenized_datasets[<span style="color:#f1fa8c">&#34;train&#34;</span>]
</span></span><span style="display:flex;"><span>full_eval_dataset <span style="color:#ff79c6">=</span> tokenized_datasets[<span style="color:#f1fa8c">&#34;test&#34;</span>]
</span></span></code></pre></div><ul>
<li>use the full ones once you have all params figured out and want to do the final training</li>
</ul>
<h3 id="import-model">Import model</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> GPT2Tokenizer, GPT2Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> GPT2Tokenizer<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#39;gpt2-medium&#39;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> GPT2Model<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#39;gpt2-medium&#39;</span>)
</span></span></code></pre></div><h3 id="training">Training</h3>
<ul>
<li><em>Transformers</em> has a <code>Trainer</code> class that can speed up training of models, and does a lot of the work for us</li>
<li><code>Trainer</code> is defined as a dict of arguments and a <code>compute_metrics</code> function, but first we need to define these:</li>
</ul>
<h5 id="training-args">Training args:</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> TrainingArguments
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>training_args <span style="color:#ff79c6">=</span> TrainingArguments(<span style="color:#f1fa8c">&#34;test_trainer&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4">#use just default args to start with</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">#add arg: evaluation_strategy=&#34;epoch&#34; to report metrics every epoch</span>
</span></span></code></pre></div><h4 id="configure-training-metrics">Configure training metrics</h4>
<ul>
<li><em>Trainer</em> can take a <code>compute_metrics()</code> function, which takes predictions and labels (in a tuple), and returns a dict with metric names and values</li>
<li>we can use the <em>Datasets</em> library to get access to common metrics
<ul>
<li>&lsquo;accuracy&rsquo; is one of these</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> numpy <span style="color:#ff79c6">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> datasets <span style="color:#ff79c6">import</span> load_metric
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>metric <span style="color:#ff79c6">=</span> load_metric(<span style="color:#f1fa8c">&#34;accuracy&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">compute_metrics</span>(eval_pred):
</span></span><span style="display:flex;"><span>    logits, labels <span style="color:#ff79c6">=</span> eval_pred <span style="color:#6272a4">#splitting tuple into the output logits and their labels</span>
</span></span><span style="display:flex;"><span>    predictions <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>argmax(logits, axis<span style="color:#ff79c6">=-</span><span style="color:#bd93f9">1</span>) <span style="color:#6272a4">#convert logits into predictions</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> metric<span style="color:#ff79c6">.</span>compute(predictions<span style="color:#ff79c6">=</span>predictions, references<span style="color:#ff79c6">=</span>labels) <span style="color:#6272a4">#calc predict accuracy</span>
</span></span></code></pre></div><h3 id="define-trainer">Define Trainer</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> Transformers <span style="color:#ff79c6">import</span> Trainer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff79c6">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#ff79c6">=</span>model,
</span></span><span style="display:flex;"><span>    args<span style="color:#ff79c6">=</span>training_args,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#ff79c6">=</span>small_train_dataset,
</span></span><span style="display:flex;"><span>    eval_dataset<span style="color:#ff79c6">=</span>small_eval_dataset,
</span></span><span style="display:flex;"><span>    compute_metrics<span style="color:#ff79c6">=</span>compute_metrics,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="train-and-evaluate">Train and Evaluate:</h3>
<pre tabindex="0"><code>trainer.train()
trainer.evaluate()
</code></pre><p>We are now done! the <a href="https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.TrainingArguments"  target="_blank" rel="noopener" >training args</a> or dataset can be tweaked to try to improve performance</p>
<p>Remember to <a href="https://discuss.huggingface.co/t/save-load-and-do-inference-with-fine-tuned-model/76291/2"  target="_blank" rel="noopener" >save your model</a>!
<code>model.save_pretrained(&quot;path/to/model.pt&quot;)</code></p>

    </article>
  </div>

</div>


  
<script type="text/javascript" src="/main.js" defer></script>


  


</body>

</html>