<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Word2Vec in detail - jwhogg
  </title>

  <meta name="description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which can&rsquo;t be easily represented numerically." /><meta name="generator" content="Hugo 0.126.1">

  <link rel="stylesheet" href="http://localhost:1313/css/main.css" />

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"

    onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>

  
  

  <meta property="og:url" content="http://localhost:1313/articles/word2vec_intro/">
  <meta property="og:site_name" content="jwhogg">
  <meta property="og:title" content="Word2Vec in detail">
  <meta property="og:description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2024-06-02T14:25:04+01:00">
    <meta property="article:modified_time" content="2024-06-02T14:25:04+01:00">
    <meta property="og:image" content="http://localhost:1313/digital-garden-logo.png">


  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/digital-garden-logo.png">
  <meta name="twitter:title" content="Word2Vec in detail">
  <meta name="twitter:description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">


  
  <meta itemprop="name" content="Word2Vec in detail">
  <meta itemprop="description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">
  <meta itemprop="datePublished" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="dateModified" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="wordCount" content="528">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">

  
  <meta itemprop="name" content="Word2Vec in detail">
  <meta itemprop="description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">
  <meta itemprop="datePublished" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="dateModified" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="wordCount" content="528">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">
</head><head>
  
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Word2Vec in detail - jwhogg
  </title>

  <meta name="description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which can&rsquo;t be easily represented numerically." /><meta name="generator" content="Hugo 0.126.1">

  <link rel="stylesheet" href="http://localhost:1313/css/main.css" />

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>





<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"

    onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>

  
  

  <meta property="og:url" content="http://localhost:1313/articles/word2vec_intro/">
  <meta property="og:site_name" content="jwhogg">
  <meta property="og:title" content="Word2Vec in detail">
  <meta property="og:description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2024-06-02T14:25:04+01:00">
    <meta property="article:modified_time" content="2024-06-02T14:25:04+01:00">
    <meta property="og:image" content="http://localhost:1313/digital-garden-logo.png">


  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/digital-garden-logo.png">
  <meta name="twitter:title" content="Word2Vec in detail">
  <meta name="twitter:description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">


  
  <meta itemprop="name" content="Word2Vec in detail">
  <meta itemprop="description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">
  <meta itemprop="datePublished" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="dateModified" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="wordCount" content="528">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">

  
  <meta itemprop="name" content="Word2Vec in detail">
  <meta itemprop="description" content="In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which canâ€™t be easily represented numerically.">
  <meta itemprop="datePublished" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="dateModified" content="2024-06-02T14:25:04+01:00">
  <meta itemprop="wordCount" content="528">
  <meta itemprop="image" content="http://localhost:1313/digital-garden-logo.png">
</head>
</head>
<body class="flex relative h-full min-h-screen"><aside
  class="will-change-transform transform transition-transform -translate-x-full absolute top-0 left-0 md:relative md:translate-x-0 w-3/4 md:basis-60 h-full min-h-screen p-3 bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 flex flex-col gap-2.5 z-20 sidebar flex-shrink-0">
  <p class="font-bold mb-5 flex items-center gap-2">
    <button aria-label="Close sidebar"
      class="md:hidden menu-trigger-close p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-6 w-6" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
</svg></button>
    <a href="http://localhost:1313/" class="px-2">
      <span>jwhogg</span>
    </a>
    <button aria-label="Toggle dark mode"
      class="dark-mode-toggle p-2 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <circle cx="12" cy="12" r="4" />
  <path d="M3 12h1M12 3v1M20 12h1M12 20v1M5.6 5.6l.7 .7M18.4 5.6l-.7 .7M17.7 17.7l.7 .7M6.3 17.7l-.7 .7" />
</svg></button>
  </p>

  
  <ul class="list-none flex flex-col gap-1">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/" >
        <span>Home</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  text-slate-400 font-semibold pb-0 pl-1 border-b cursor-default pointer-events-none "
        href="#" >
        <span>Content</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles" >
        <span>Articles</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/portfolio" >
        <span>Portfolio</span>
        
      </a>
    </li>
    
  </ul>

  <div class="flex-1"></div>

  

  <ul class="list-none flex flex-wrap justify-center gap-1 pt-2 border-t border-slate-200 dark:border-slate-600">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="https://github.com/jwhogg" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">GitHub</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path
    d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22" />
</svg></span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="https://linkedin.com/in/joel-w-hogg" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">LinkedIn</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z" />
  <rect x="2" y="9" width="4" height="12" />
  <circle cx="4" cy="4" r="2" />
</svg></span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="mailto:joelhogg45@gmail.com" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">Email</span>
        
        <span><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <rect x="3" y="5" width="18" height="14" rx="2" />
  <polyline points="3 7 12 13 21 7" />
</svg></span>
        
      </a>
    </li>
    
  </ul>
</aside>

<div
  class="fixed bg-slate-700 bg-opacity-5 transition duration-200 ease-in-out inset-0 z-10 pointer-events-auto md:hidden left-0 top-0 w-full h-full hidden menu-overlay">
</div>

<button aria-label="Toggle Sidebar"
  class="md:hidden absolute top-3 left-3 z-10 menu-trigger p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-100"><svg class="h-6 w-6" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="4" y1="6" x2="20" y2="6" />
  <line x1="4" y1="12" x2="20" y2="12" />
  <line x1="4" y1="18" x2="16" y2="18" />
</svg></button>






<div class="flex flex-1 h-screen relative w-full min-w-0">

  <section
    class="will-change-transform transform transition-transform -translate-x-[200%] absolute top-0 left-0 lg:relative
  lg:translate-x-0 lg:basis-[400px] h-full bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 lg:flex flex-col py-3 overflow-y-auto scroll-area flex-shrink-0">
    
    
    <a href="http://localhost:1313/articles/">
      <h2 class="font-bold mb-5 py-1 pl-12 pr-3 md:px-3">Articles</h2>
    </a>
<label class="mb-2 block px-2">
  <span></span>
  <input type="text" placeholder="Search articles" class="searchInput bg-white rounded px-2 py-1 w-full border" />
</label>
<script>
  const searchElt = document.querySelector('.searchInput')
  if (searchElt) {
    searchElt.addEventListener('input', evt => {
      const value = evt.target.value || '';

      const articles = document.querySelectorAll('.article')
      if (!value) {
        articles.forEach(article => article.classList.remove('hidden'))
      } else {
        articles.forEach(article => article.classList.add('hidden'))
        Array.from(articles).filter(article => {
          const title = article.querySelector('h3')
          if (!title) return false;

          return title.textContent.toLowerCase().includes(value.toLowerCase())
        }).forEach(article => article.classList.remove('hidden'))
      }
    })
  }
</script>
<div class="space-y-2.5">
      
      <a class="article block px-3 py-4  bg-slate-900 dark:bg-slate-700 text-slate-50 "
        href="/articles/word2vec_intro/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Word2Vec in detail</h3>
        <div
          class="text-sm  text-slate-400  line-clamp-2">
          In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.
Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements These 2 papers introduced word2vec to the world back in 2013
Motivation For many NLP tasks, we need to learn on data which can&rsquo;t be easily represented numerically.
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/word2vec_code_overview/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Implementing Word2Vec in python</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.
Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/finetuning_pretrained_model/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Fine-tuning a pre-trained model from HuggingFace</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)
ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)
Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;your-model-here&#34;) #eg &#34;bert-base-cased&#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&#34;train&#34;].
        </div>
      </a>
      
      <a class="article block px-3 py-4  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/articles/getting_an_ml_jpb/">
        
        
        <h3 class="text-lg font-semibold mb-0.5">Getting an ML job- notes from blogs</h3>
        <div
          class="text-sm  text-slate-500 dark:text-slate-400  line-clamp-2">
          Summary of this article / video
Intro can split getting an ML job into 2 steps: Getting ML skills building projects contributing to OS reading technical info Marketing ML skills communication interviewing portal creation passing application screening Strategies: Make ML jobs come to you by Learning in Public (great article!) Summary:
write blogs / tutorials / cheatsheets answer things on stackoverflow / reddit make videos draw cartoons Pull Request libraries you use OS: github repo -&gt; issues -&gt; &lsquo;good first issue&rsquo; (easy ones to solve) make your own libraries intended only for you goto conferences ==&gt; make the thing you wish you had found Info
        </div>
      </a>
      </div>
    </section>

  <div class="overflow-y-auto h-screen w-full">
    <article class="px-6 py-20 w-full mx-auto prose lg:prose-lg h-fit dark:prose-invert prose-img:mx-auto">

      
      <h1 class="!mb-2">Word2Vec in detail</h1>

	  
      <p class="text-sm text-slate-500 !mb-8"> Planted June 2, 2024
		
	  </p>
      

      

      <p>In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released.</p>
<table>
<thead>
<tr>
<th><img src="/images/word2vec_paper_1.png" alt="word2vec paper 1" width="200px" height="150px"></th>
<th><img src="/images/word2vec_paper_2.png" alt="word2vec paper 2" width="200px" height="150px"></th>
</tr>
</thead>
<tbody>
<tr>
<td>Word2Vec Paper 1- introducing CBOW and Skip-Gram</td>
<td>Word2Vec Paper 2- Performance Improvements</td>
</tr>
</tbody>
</table>
<p><em>These 2 papers introduced word2vec to the world back in 2013</em></p>
<h3 id="motivation">Motivation</h3>
<p>For many NLP tasks, we need to learn on data which can&rsquo;t be easily represented numerically. For example, let&rsquo;s look at the popular <a href="https://huggingface.co/datasets/stanfordnlp/imdb/viewer/plain_text/train"  target="_blank" rel="noopener" >IMDB dataset</a>, which gives reviews in one column, and a binary sentiment label in the next.
<img src="/images/imdb.png" alt="IMDB dataset"/>
Neural Networks accept vectors, so we need a way to represent these strings as such.</p>
<p><strong>The &lsquo;old&rsquo; way:</strong></p>
<p>We can reprent a corpus of words using a <a href="https://en.wikipedia.org/wiki/Bag-of-words_model"  target="_blank" rel="noopener" >Bag-of-Words</a>. For some input text, we can create an unordered &lsquo;bag&rsquo; of the text&rsquo;s vocabulary (set of unique words), and how many count times it appears in the text.</p>
<p>Raw text:</p>
<pre tabindex="0"><code>&#34;Ogres are like onions, onions have layers.
</code></pre><p>Bag-of-Words:</p>
<pre tabindex="0"><code>{&#34;Ogres&#34;: 1, &#34;are&#34;: 1, &#34;onions&#34;: 2, &#34;like&#34;: 1, &#34;have&#34;: 1, &#34;layers&#34;: 1}
</code></pre><p>We can use a BOW to represent any word from our corpus using <a href="https://www.geeksforgeeks.org/ml-one-hot-encoding/"  target="_blank" rel="noopener" >one-hot encoding</a>:</p>
<p>Onions:  <pre>[0,0,1,0,0,0]</pre></p>
<p>Ogres: <pre>[1,0,0,0,0,0]</pre></p>
<p>Although an ML model <em>could</em> learn using this method, it is grossly inefficient: imagine a corpus with a vocabulary (number of unique words) of 100,000. This would mean you would need a combined vector size of 1,000,000 to encode just 10 words, where 99.9% of the vector is empty.</p>
<p><strong>The &rsquo;new&rsquo; way</strong>:</p>
<p>word2vec creates a <strong>distributed representation</strong> of words, which is much more efficient, and allows much a much larger corpus of text to be used in training.</p>
<p>Each vector is continious and dense, where vectors that are closer have similar meanings. This distributed representation of a large text corpus is our goal, so we can use it in other NLP tasks, and word2vec is how we will accomplish it. Since 2013, there are better ways to do this, such as <a href="https://arxiv.org/abs/1810.04805"  target="_blank" rel="noopener" >BERT</a> and <a href="https://nlp.stanford.edu/projects/glove/"  target="_blank" rel="noopener" >GloVe</a>, which can embed more nuanced relationships between words.</p>
<p>The famous example given in the paper is that using vector math with the final embeddings, you could do something like: $\vec{King} - \vec{Man} + \vec{Woman} = \vec{Queen}$. In reality, the resultant vector may not line up exactly with $\vec{Queen}$, but Queen should be the closest vector using <a href="https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded"  target="_blank" rel="noopener" >cosine distance</a>.</p>
<p>see <a href="https://www.youtube.com/watch?v=FJtFZwbvkI4"  target="_blank" rel="noopener" >here</a> for a great explination from 3b1b on how vectors can encode meaning.</p>
<p><strong>&lsquo;Closeness&rsquo;</strong>: If every word is a vector, &lsquo;closeness&rsquo; between any 2 words (vectors) can be defined as a dot-product of those vectors being close to 1.</p>
<p><strong>Distributional similarity</strong>- representing a word by means of its neighbors. For a word, its meaning must be encoded by all of the words that are near it in every context. For example, if we look for the word banking across a massive corpus of text, and note all the words nearby, these words must somehow encode the meaning of &lsquo;banking&rsquo;.</p>
<h3 id="how-word2vec-encodes-words">How Word2Vec encodes words</h3>
<p>Now we understand why we need to encode words as dense vectors that embed meaning, we can jump in to how word2vec accomplishes this. The paper establishes 2 &lsquo;fake&rsquo; tasks that get the model to learn embeddings: &hellip;</p>

    </article>
  </div>

</div>


  
<script type="text/javascript" src="/main.js" defer></script>


  


</body>

</html>