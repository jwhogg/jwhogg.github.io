<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width"><meta name="description" content="" />

<title>
    
    Word2Vec Overview | jwhogg
    
</title>







<link rel="stylesheet" href="/assets/combined.min.020f42c86d32ba80b53d3fc40ff0090ec5f74817ba9a0391ba123948ca68c7fc.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">
    <h1 class="header-title">jwhogg</h1>
    

    <div class="flex">
        

        
        
        <p class="small ">
            <a href="/">
                /home
            </a>
        </p>

        <p>        </p>
        
        <p class="small ">
            <a href="/articles">
                /articles
            </a>
        </p>

        <p>        </p>
        
        <p class="small ">
            <a href="/portfolio">
                /portfolio
            </a>
        </p>

        <p>        </p>
        
        
        
        <div class="social-icons">
            <a href="www.linkedin.com/in/joel-w-hogg" target="_blank"
            rel="noopener noreferrer me"
            title="Linkedin">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
            </a>
            <a href="https://github.com/jwhogg" target="_blank"
            rel="noopener noreferrer me"
            title="Github">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
            </a>
            <a href="mailto:joelhogg45@gmail.com" target="_blank"
            rel="noopener noreferrer me"
            title="Email">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
    <polyline points="22,6 12,13 2,6"></polyline>
</svg>
            </a>
        </div>
    
    </div>

</div>
      </header>

      <main class="main">
        




<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a href="/articles/">Articles</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/articles/word2vec_intro/">Word2Vec Overview</a>
</div>


<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Word2Vec Overview</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-06-12T00:00:00&#43;00:00">June 12, 2024</time>
      

      
      &nbsp; Â· &nbsp;
      5 min read
      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <p>In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released. We will also go into the training of the neural network, so it is assumed you have some knowledge on this.</p>
<div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        These 2 papers introduced word2vec to the world back in 2013:
    </p>
</div>
<table>
<thead>
<tr>
<th>













<figure class=" img-smaller">

    <div>
        <img loading="lazy" alt="paper1" src=" /images/word2vec_paper_1.webp#smaller">
    </div>

    
</figure></th>
<th>













<figure class=" img-smaller">

    <div>
        <img loading="lazy" alt="paper2" src=" /images/word2vec_paper_2.webp#smaller">
    </div>

    
</figure></th>
</tr>
</thead>
<tbody>
<tr>
<td><div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        <a href="https://arxiv.org/pdf/1301.3781">Word2Vec Paper 1</a>- introducing CBOW and Skip-Gram
    </p>
</div></td>
<td><div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        <a href="https://arxiv.org/pdf/1310.4546">Word2Vec Paper 2</a>- Performance Improvements
    </p>
</div></td>
</tr>
</tbody>
</table>
<h3 id="motivation">Motivation</h3>
<p>For many NLP tasks, we need to learn on data which can&rsquo;t be easily represented numerically. For example, let&rsquo;s look at the popular <a href="https://huggingface.co/datasets/stanfordnlp/imdb/viewer/plain_text/train">IMDB dataset</a>, which gives reviews in one column, and a binary sentiment label in the next:</p>
<!-- raw HTML omitted -->
<p>













<figure class=" img-small">

    <div>
        <img loading="lazy" alt="imdb" src=" /images/imdb.webp#small">
    </div>

    
</figure>
Neural Networks accept vectors, so we need a way to represent these strings as such.</p>
<h4 id="the-old-way">The &lsquo;old&rsquo; way:</h4>
<p>We can reprent a corpus of words using a <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-Words</a>. For some input text, we can create an unordered &lsquo;bag&rsquo; of the text&rsquo;s vocabulary (set of unique words), and how many count times it appears in the text.</p>
<p>Raw text:</p>
<pre tabindex="0"><code>&#34;Ogres are like onions, onions have layers.
</code></pre><p>Bag-of-Words:</p>
<pre tabindex="0"><code>{&#34;Ogres&#34;: 1, &#34;are&#34;: 1, &#34;onions&#34;: 2, &#34;like&#34;: 1, &#34;have&#34;: 1, &#34;layers&#34;: 1}
</code></pre><p>We can use a BOW to represent any word from our corpus using <a href="https://www.geeksforgeeks.org/ml-one-hot-encoding/">one-hot encoding</a>:</p>
<pre tabindex="0"><code>Onions:  [0,0,1,0,0,0]
</code></pre><pre tabindex="0"><code>Ogres: [1,0,0,0,0,0]
</code></pre><p>Although an ML model <em>could</em> learn using this method, it is grossly inefficient: imagine a corpus with a vocabulary (number of unique words) of 100,000. This would mean you would need a combined vector size of 1,000,000 to encode just 10 words, where 99.9% of the vector is empty.</p>
<h4 id="the-new-way">The &rsquo;new&rsquo; way:</h4>
<p>word2vec creates a <strong>distributed representation</strong> of words, which is much more efficient, and allows much a much larger corpus of text to be used in training.</p>
<p>Each vector is continious and dense, where vectors that are closer have similar meanings. This distributed representation of a large text corpus is our goal, so we can use it in other NLP tasks, and word2vec is how we will accomplish it. Since 2013, there are better ways to do this, such as <a href="https://arxiv.org/abs/1810.04805">BERT</a> and <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, which can embed more nuanced relationships between words.</p>
<p>Dense embedding of &lsquo;Ogre&rsquo;:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>array([ 7.10577386e-01, -5.09895974e-01,  1.71235001e+00,  2.05854094e+00, -4.47590043e-01, -1.00782587e+00, -1.14494191e+00, -7.94087996e-01, -2.22047371e-01,...])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>array_shape: (400,)
</span></span></code></pre></div><p>The famous example given in the paper is that using vector math with the final embeddings, you could do something like: $\vec{King} - \vec{Man} + \vec{Woman} = \vec{Queen}$. In reality, the resultant vector may not line up exactly with $\vec{Queen}$, but Queen should be the closest vector using <a href="https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded">cosine distance</a>.</p>
<div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        see <a href="https://www.youtube.com/watch?v=FJtFZwbvkI4">here</a> for a great explination from 3b1b on how vectors can encode meaning.
    </p>
</div>
<p><strong>&lsquo;Closeness&rsquo;</strong>: If every word is a vector, &lsquo;closeness&rsquo; between any 2 words (vectors) can be defined as a dot-product of those vectors being close to 1.</p>
<p><strong>Distributional similarity</strong>- representing a word by means of its neighbors. For a word, its meaning must be encoded by all of the words that are near it in every context. For example, if we look for the word &lsquo;banking&rsquo; across a massive corpus of text, and note all the words nearby, these words must somehow encode the meaning of &lsquo;banking&rsquo;.</p>
<h2 id="how-word2vec-encodes-words">How Word2Vec encodes words</h2>
<p>Now we understand why we need to encode words as dense vectors that embed meaning, we can jump in to how word2vec accomplishes this. The paper establishes 2 &lsquo;fake&rsquo; tasks that get the model to learn embeddings:</p>
<!-- raw HTML omitted -->
<p>













<figure class=" img-small">

    <div>
        <img loading="lazy" alt="cbow and skipgram" src=" /images/cbow_skipgram.webp#small">
    </div>

    
</figure></p>
<h4 id="continuous-bag-of-words-cbow">Continuous Bag-of-Words (CBOW)</h4>
<p>The neural network is passed a few context words from around the target word, with the goal of predicting what that target word is.</p>
<h4 id="skip-gram">Skip-gram</h4>
<p>The opposite of CBOW, the model is instead passed the middle word, with the goal of predicting the context words around it.</p>
<p>note: <strong>These words will be passed to the model as one-hot encodings</strong></p>
<h3 id="retrieving-embeddings">Retrieving embeddings</h3>
<p>













<figure class=" img-medium">

    <div>
        <img loading="lazy" alt="word2vec network" src=" /images/word2vecnetwork.webp#medium">
    </div>

    
</figure>
<div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        A matrix-focused view of the model architecture, imagine data flowing left-&gt;right
    </p>
</div>
With the above representation of our word2vec model, the matrix $w_{1}$ (representing weights for the input -&gt; hidden layer) <strong>is actually the embeddings table for our words!</strong> This is the &lsquo;synthetic task&rsquo; that we have been training for, where the real goal is actually getting good embedding vectors.</p>
<p>













<figure class=" img-smaller">

    <div>
        <img loading="lazy" alt="the hidden layer is the word vector matrix" src=" /images/hiddenlayeriswordvector.webp#smaller">
    </div>

    
</figure>
<div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        credit: Chris McCormik
    </p>
</div></p>
<h3 id="softmax-activation">Softmax activation</h3>
<p>Although later word2vec papers go on to describe the more efficient <a href="https://paperswithcode.com/method/hierarchical-softmax">hierarchical softmax</a>, we will focus just on how softmax helps us clarify which word the network is predicting.</p>
<p>$$  \text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}  $$</p>
<p>













<figure class=" img-smaller">

    <div>
        <img loading="lazy" alt="softmax demo" src=" /images/softmax.webp#smaller">
    </div>

    
</figure>
<div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        Where the softmax gets applied (imagine data flow left-&gt;right)
    </p>
</div></p>
<p>The <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax activation</a> can take a vector of numbers, and maps each number to between $[0,1]$, where the sum of the vector will always add up to 1. From this, we can find our model&rsquo;s prediction by getting the element with the highest number.</p>
<h2 id="training">Training</h2>
<p>Using a context window, we can generate pairs for the model in the form: <strong>(TARGET,CONTEXT)</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>













<figure class=" img-small">

    <div>
        <img loading="lazy" alt="ogres are like onions" src=" /images/ogresonions1.webp#small">
    </div>

    
</figure></p>
<p>Becomes the following pairs:</p>
<pre tabindex="0"><code>(onions,ogres), (onions,are), (onions,like), (onions,they), (onions,have), (onions,layers)
</code></pre><p>And using the next target word:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>













<figure class=" img-small">

    <div>
        <img loading="lazy" alt="ogres are like onions" src=" /images/ogresonions2.webp#small">
    </div>

    
</figure>
<div style="display: flex; justify-content: center;">
    <p style="text-align: center; font-size: small; color: gray;">
        Let&rsquo;s assume our corpus ends at &rsquo;layers&rsquo;, otherwise with context = 3 there would be another word on the right
    </p>
</div></p>
<p>Becomes these pairs:</p>
<pre tabindex="0"><code>(they,ogres), (they,are), (they,like), (they,onions), (they,have), (they,layers)
</code></pre><p>Using a training pair, $(Y,X)$ (both one-hot encoded), the model will get passed $X$, and have its output compared against $Y$ to learn from.</p>
<h4 id="distinction-between-cbow-and-skip-gram-training">Distinction between CBOW and Skip-gram training</h4>
<p>It&rsquo;s important to make a distinction regarding these pairs for the 2 models, because they <em>are</em> slightly different:</p>
<ul>
<li>For Skip-gram, the model would be passed a target word ($Y$), and for output, predicts <strong>all</strong> the words within the context window ($ X_{i-n} $). If we had 10 words in the vocabularly, and a context size 3, this would make the model&rsquo;s output vector shape (6,10)- one row for each context word.</li>
<li>For CBOW, the model is passed all words in the context window ($X_{i-n}$), and for output, predicts the target word that fits the best ($Y$). Inside the neural network, the projection layer, which maps input to the hidden layer, a<strong>verages all the word vectors into one</strong>, so after this step, the network is working with one average word vector.</li>
</ul>
<p>That&rsquo;s just about it! I hope you&rsquo;ve learned something from this article- in the next one in this series, I will be going through building the word2vec CBOW model in python within numpy.</p>
<p><strong>Sources / Further Reading:</strong></p>
<ul>
<li>
<p><a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Chris McCormik- Word2Vec guide</a></p>
</li>
<li>
<p><a href="https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/">Word2Vec from scratch with python and numpy</a></p>
</li>
<li>
<p><a href="https://jaketae.github.io/study/word2vec/">Jake Tae- Word2Vec from scratch</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1301.3781">Word2Vec original paper</a></p>
</li>
</ul>

    
  </div>

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      <p>Powered by
    <a href="https://gohugo.io/">Hugo</a>
    and
    <a href="https://github.com/tomfran/typo">tomfran/typo</a>
</p>


<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ]
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>