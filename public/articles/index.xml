<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Articles on jwhogg</title>
    <link>http://localhost:1313/articles/</link>
    <description>Recent content in Articles on jwhogg</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 May 2024 17:25:04 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/articles/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Implementing Word2Vec in python</title>
      <link>http://localhost:1313/articles/word2vec_code_overview/</link>
      <pubDate>Fri, 31 May 2024 17:25:04 +0100</pubDate>
      <guid>http://localhost:1313/articles/word2vec_code_overview/</guid>
      <description>We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.&#xA;Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:</description>
    </item>
    <item>
      <title>Fine-tuning a pre-trained model from HuggingFace</title>
      <link>http://localhost:1313/articles/finetuning_pretrained_model/</link>
      <pubDate>Fri, 24 May 2024 13:29:04 +0100</pubDate>
      <guid>http://localhost:1313/articles/finetuning_pretrained_model/</guid>
      <description>We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)&#xA;ðŸ“™Jupyter Notebook Create train/test split for custom dataset (can use sklearn for this)&#xA;Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&amp;#34;your-model-here&amp;#34;) #eg &amp;#34;bert-base-cased&amp;#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&amp;#34;text&amp;#34;], padding=&amp;#34;max_length&amp;#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&amp;#34;train&amp;#34;].</description>
    </item>
    <item>
      <title>Getting an ML job- notes from blogs</title>
      <link>http://localhost:1313/articles/getting_an_ml_jpb/</link>
      <pubDate>Thu, 23 May 2024 13:07:04 +0100</pubDate>
      <guid>http://localhost:1313/articles/getting_an_ml_jpb/</guid>
      <description>Summary of this article / video&#xA;Intro can split getting an ML job into 2 steps: Getting ML skills building projects contributing to OS reading technical info Marketing ML skills communication interviewing portal creation passing application screening Strategies: Make ML jobs come to you by Learning in Public (great article!) Summary:&#xA;write blogs / tutorials / cheatsheets answer things on stackoverflow / reddit make videos draw cartoons Pull Request libraries you use OS: github repo -&amp;gt; issues -&amp;gt; &amp;lsquo;good first issue&amp;rsquo; (easy ones to solve) make your own libraries intended only for you goto conferences ==&amp;gt; make the thing you wish you had found Info</description>
    </item>
  </channel>
</rss>
