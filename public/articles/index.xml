<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Articles on jwhogg</title>
    <link>http://localhost:1313/articles/</link>
    <description>Recent content in Articles on jwhogg</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/articles/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Edge AI: ML inference in the browser</title>
      <link>http://localhost:1313/articles/ml_inference_browser/</link>
      <pubDate>Tue, 18 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/ml_inference_browser/</guid>
      <description>Recently, I stumbled across a guy who ported OpenAI&amp;rsquo;s Whisper model into c++, in various sizes, allowing the model to be run on-device, at impressive speed.&#xA;(source) {{ youtube TCMdywaJ9iY}} I went down a rabbit-hole, and found a whole family of popular models that had been ported to work on-device, from the browser:&#xA;Stable difusion running in browser Text emotion prediction in browser Llama c++ Web LLM YOLO in the browser Edge Computing This movement is part of the larger Edge computing trend, which focuses on bringing computing as close to the source as possible, to reduce latency.</description>
    </item>
    <item>
      <title>Word2Vec Overview</title>
      <link>http://localhost:1313/articles/word2vec_intro/</link>
      <pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/word2vec_intro/</guid>
      <description>In this article we will introduce the context surrounding word2vec, including the motivation for distributed word embeddings, how the Continious Bag-of-Words and Skip-gram algorithms work, and the advancements since the original paper was released. We will also go into the training of the neural network, so it is assumed you have some knowledge on this.&#xA;These 2 papers introduced word2vec to the world back in 2013: Word2Vec Paper 1- introducing CBOW and Skip-Gram Word2Vec Paper 2- Performance Improvements Motivation For many NLP tasks, we need to learn on data which can&amp;rsquo;t be easily represented numerically.</description>
    </item>
    <item>
      <title>Implementing Word2Vec in python</title>
      <link>http://localhost:1313/articles/word2vec_code_overview/</link>
      <pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/word2vec_code_overview/</guid>
      <description>We will be implementing the Neural Network for the Continuous Bag of Words (CBOW) from the word2vec paper. This article assumes you have a good understanding of the high-level of word2vec. This will be covered in coming articles also.&#xA;Our goal is to train with sample pairs $(y,X)$, where $y$ is the target word, and $X$ is one of the context words from within the window. The error function we want to minimise:</description>
    </item>
    <item>
      <title>Fine-tuning a pre-trained model from HuggingFace</title>
      <link>http://localhost:1313/articles/finetuning_pretrained_model/</link>
      <pubDate>Fri, 24 May 2024 13:29:04 +0100</pubDate>
      <guid>http://localhost:1313/articles/finetuning_pretrained_model/</guid>
      <description>We will be using a ðŸ¤— HuggingFace model (GPT-2 Medium)&#xA;ðŸ“™Jupyter Notebook Link&#xA;Create train/test split for custom dataset (can use sklearn for this)&#xA;Get the model tokeniser from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&amp;#34;your-model-here&amp;#34;) #eg &amp;#34;bert-base-cased&amp;#34; Create encodings for train/test using tokeniser def tokenize_function(examples): return tokenizer(examples[&amp;#34;text&amp;#34;], padding=&amp;#34;max_length&amp;#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # where raw_datasets is a dict with train/dev/test we need padding as the inputs must fit the models input even if they are too short Create small datasets for development small_train_dataset = tokenized_datasets[&amp;#34;train&amp;#34;].</description>
    </item>
    <item>
      <title>Notes on: Getting an ML job</title>
      <link>http://localhost:1313/articles/getting_an_ml_jpb/</link>
      <pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/getting_an_ml_jpb/</guid>
      <description>Summary of this article / video&#xA;Intro can split getting an ML job into 2 steps: Getting ML skills building projects contributing to OS reading technical info Marketing ML skills communication interviewing portal creation passing application screening Strategies: Make ML jobs come to you by Learning in Public (great article!) Summary:&#xA;write blogs / tutorials / cheatsheets answer things on stackoverflow / reddit make videos draw cartoons Pull Request libraries you use OS: github repo -&amp;gt; issues -&amp;gt; &amp;lsquo;good first issue&amp;rsquo; (easy ones to solve) make your own libraries intended only for you goto conferences ==&amp;gt; make the thing you wish you had found Info</description>
    </item>
  </channel>
</rss>
